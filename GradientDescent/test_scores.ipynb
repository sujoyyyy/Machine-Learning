{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_using_sklean():\n",
    "    df = pd.read_csv(\"test_scores.csv\")\n",
    "    r = LinearRegression()\n",
    "    r.fit(df[['math']],df.cs)\n",
    "    return r.coef_, r.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x,y):\n",
    "    m_curr = 0\n",
    "    b_curr = 0\n",
    "    iterations = 100\n",
    "    n = len(x)\n",
    "    learning_rate = 0.0002\n",
    "\n",
    "    cost_previous = 0\n",
    "\n",
    "    for i in range(iterations):\n",
    "        y_predicted = m_curr * x + b_curr\n",
    "        cost = (1/n)*sum([value**2 for value in (y-y_predicted)])\n",
    "        md = -(2/n)*sum(x*(y-y_predicted))\n",
    "        bd = -(2/n)*sum(y-y_predicted)\n",
    "        m_curr = m_curr - learning_rate * md\n",
    "        b_curr = b_curr - learning_rate * bd\n",
    "        if math.isclose(cost, cost_previous, rel_tol=1e-20):\n",
    "            break\n",
    "        cost_previous = cost\n",
    "        print (\"m {}, b {}, cost {}, iteration {}\".format(m_curr,b_curr,cost, i))\n",
    "\n",
    "    return m_curr, b_curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m 1.9783600000000003, b 0.027960000000000002, cost 5199.1, iteration 0\n",
      "m 0.20975041279999962, b 0.0030470367999999894, cost 4161.482445460163, iteration 1\n",
      "m 1.7908456142986242, b 0.025401286955264, cost 3332.2237319269248, iteration 2\n",
      "m 0.37738163667530467, b 0.005499731626422651, cost 2669.4843523161976, iteration 3\n",
      "m 1.6409848166378898, b 0.023373894401807944, cost 2139.826383775145, iteration 4\n",
      "m 0.5113514173939655, b 0.0074774305434828076, cost 1716.5264071567592, iteration 5\n",
      "m 1.5212165764726306, b 0.021771129698498662, cost 1378.2272007804495, iteration 6\n",
      "m 0.6184191426785134, b 0.009075514323270572, cost 1107.8601808918404, iteration 7\n",
      "m 1.4254981563597626, b 0.020507724625171385, cost 891.7842215178443, iteration 8\n",
      "m 0.7039868810749315, b 0.010370210797388455, cost 719.0974036421305, iteration 9\n",
      "m 1.3490002310389348, b 0.01951553325074733, cost 581.0869686205, iteration 10\n",
      "m 0.7723719384951477, b 0.01142244086408669, cost 470.7897237271261, iteration 11\n",
      "m 1.2878632281408475, b 0.018740093691150705, cost 382.6407204862143, iteration 12\n",
      "m 0.8270246840299113, b 0.012280892197750798, cost 312.1924801681589, iteration 13\n",
      "m 1.2390025969113474, b 0.01813788028359247, cost 255.89060022344475, iteration 14\n",
      "m 0.8707026352388424, b 0.012984475742007832, cost 210.89442007737276, iteration 15\n",
      "m 1.1999531799587442, b 0.01767410753812916, cost 174.93369813849728, iteration 16\n",
      "m 0.9056095862354473, b 0.013564288926616264, cost 146.19406878727372, iteration 17\n",
      "m 1.168744835939885, b 0.017320975066834464, cost 123.2255001796068, iteration 18\n",
      "m 0.9335067981503328, b 0.014045184660493999, cost 104.86913418555842, iteration 19\n",
      "m 1.1438030378387343, b 0.017056264940052912, cost 90.1988172376793, iteration 20\n",
      "m 0.9558018619881088, b 0.014447025263025912, cost 78.4743720801518, iteration 21\n",
      "m 1.123869431612398, b 0.016862220700598438, cost 69.10425278659366, iteration 22\n",
      "m 0.9736197173740411, b 0.014785684599634922, cost 61.61569883880534, iteration 23\n",
      "m 1.1079383470620547, b 0.016724651477560692, cost 55.63088241716976, iteration 24\n",
      "m 0.9878594103778675, b 0.015073848983471565, cost 50.84784543555072, iteration 25\n",
      "m 1.0952060576414993, b 0.016632215998581557, cost 47.02526451581355, iteration 26\n",
      "m 0.9992394540800741, b 0.015321657252001263, cost 43.970275232370476, iteration 27\n",
      "m 1.0850302291522722, b 0.01657585037608088, cost 41.528741309884765, iteration 28\n",
      "m 1.0083340805074807, b 0.015537212312981736, cost 39.57747781519814, iteration 29\n",
      "m 1.0768975113455124, b 0.01654831079689666, cost 38.01803597157669, iteration 30\n",
      "m 1.0156022129971571, b 0.01572698996942581, cost 36.77173601363096, iteration 31\n",
      "m 1.0703976372937574, b 0.016543808042154003, cost 35.775697470042715, iteration 32\n",
      "m 1.0214106207634122, b 0.015896165650447946, cost 34.97966658555935, iteration 33\n",
      "m 1.0652027237396349, b 0.016557715397389393, cost 34.34348081266759, iteration 34\n",
      "m 1.0260524239108442, b 0.016048875532907396, cost 33.83504244613808, iteration 35\n",
      "m 1.0610507280390304, b 0.01658633521579648, cost 33.42869916198312, iteration 36\n",
      "m 1.0297618825473565, b 0.016188425228507268, cost 33.103949752366304, iteration 37\n",
      "m 1.0577322270335765, b 0.0166267123567505, cost 32.84440975547639, iteration 38\n",
      "m 1.0327262161686235, b 0.016317456565470633, cost 32.636984792143764, iteration 39\n",
      "m 1.0550798507922887, b 0.016676485086818824, cost 32.47120990063734, iteration 40\n",
      "m 1.035095049650491, b 0.016438080879614143, cost 32.33872153636954, iteration 41\n",
      "m 1.052959838111218, b 0.01673376592060118, cost 32.23283559672576, iteration 42\n",
      "m 1.036987962438417, b 0.016551985539901195, cost 32.14821018063812, iteration 43\n",
      "m 1.0512652877114044, b 0.01679704638933073, cost 32.08057606773955, iteration 44\n",
      "m 1.0385005218215662, b 0.016660519083126275, cost 32.02652131866401, iteration 45\n",
      "m 1.0499107646303472, b 0.016865120932420777, cost 31.983319128693804, iteration 46\n",
      "m 1.0397091046950075, b 0.01676475925312493, cost 31.94879024926364, iteration 47\n",
      "m 1.0488279896772978, b 0.01693702607197308, cost 31.921193035921274, iteration 48\n",
      "m 1.0406747510877237, b 0.016865567377366896, cost 31.899135575212195, iteration 49\n",
      "m 1.047962394467687, b 0.017011991801351975, cost 31.881505456930224, iteration 50\n",
      "m 1.0414462438827428, b 0.016963631824454838, cost 31.86741364845402, iteration 51\n",
      "m 1.0472703682240316, b 0.01708940273517817, cost 31.85614963940047, iteration 52\n",
      "m 1.0420625701139212, b 0.017059502735137972, cost 31.847145593458148, iteration 53\n",
      "m 1.046717057433117, b 0.017168767060599943, cost 31.839947698713402, iteration 54\n",
      "m 1.0425548880219075, b 0.017153619779162816, cost 31.83419327097573, iteration 55\n",
      "m 1.0462746073431242, b 0.01724969172330578, cost 31.829592454870966, iteration 56\n",
      "m 1.0429480991153375, b 0.017246334338408182, cost 31.825913599447194, iteration 57\n",
      "m 1.0459207565770121, b 0.017331862596311, cost 31.82297157043309, iteration 58\n",
      "m 1.0432621045542085, b 0.017337927235534713, cost 31.82061840945363, iteration 59\n",
      "m 1.0456377139546258, b 0.017415028630952047, cost 31.81873586892226, iteration 60\n",
      "m 1.0435128092451273, b 0.017428622902632068, cost 31.81722944596486, iteration 61\n",
      "m 1.0454112608545958, b 0.01749898919044121, cost 31.816023614361388, iteration 62\n",
      "m 1.0437129243091645, b 0.017518600704730235, cost 31.81505801393808, iteration 63\n",
      "m 1.0452300338265001, b 0.017583583926907467, cost 31.81428440514876, iteration 64\n",
      "m 1.0438726084101126, b 0.01760800398949262, cost 31.81366423519171, iteration 65\n",
      "m 1.0450849512581242, b 0.017668684691178615, cost 31.813166692862268, iteration 66\n",
      "m 1.043999980300792, b 0.017696947319685068, cost 31.812767154001268, iteration 67\n",
      "m 1.0449687551708302, b 0.017754189067120032, cost 31.812445939105075, iteration 68\n",
      "m 1.0441015284474728, b 0.017785522253328603, cost 31.81218732041362, iteration 69\n",
      "m 1.0448756450247292, b 0.017840015204310798, cost 31.811978728380467, iteration 70\n",
      "m 1.0441824383996428, b 0.01787380196316831, cost 31.81181011748764, iteration 71\n",
      "m 1.0448009850576234, b 0.01792609768834459, cost 31.811673459407757, iteration 72\n",
      "m 1.0442468544222752, b 0.017961844928529556, cost 31.811562337942128, iteration 73\n",
      "m 1.0447410703917646, b 0.01801238424039495, cost 31.811471625297237, iteration 74\n",
      "m 1.0442980885910158, b 0.018049697885830843, cost 31.811397223366267, iteration 75\n",
      "m 1.044692940107559, b 0.01809883307952457, cost 31.811335856963026, iteration 76\n",
      "m 1.0443387879000003, b 0.018137398186618782, cost 31.811284908575193, iteration 77\n",
      "m 1.044654227853013, b 0.018185410814656123, cost 31.811242286300104, iteration 78\n",
      "m 1.04437106781358, b 0.01822497568209775, cost 31.811206318299767, iteration 79\n",
      "m 1.044623042451559, b 0.018272090759846052, cost 31.811175668449643, iteration 80\n",
      "m 1.0443966190001837, b 0.018312454229236455, cost 31.81114926892579, iteration 81\n",
      "m 1.044597872484431, b 0.018358851587859853, cost 31.81112626632863, iteration 82\n",
      "m 1.0444167926334849, b 0.018399852894440714, cost 31.811105978625633, iteration 83\n",
      "m 1.0445775100333783, b 0.018445676254116222, cost 31.811087860739732, iteration 84\n",
      "m 1.0444326685646632, b 0.01848718691552271, cost 31.81107147704813, iteration 85\n",
      "m 1.0445609897362342, b 0.0185325511367087, cost 31.81105647940329, iteration 86\n",
      "m 1.044445109805328, b 0.018574468470501836, cost 31.81104258956744, iteration 87\n",
      "m 1.044547540080427, b 0.01861946534911527, cost 31.811029585174346, iteration 88\n",
      "m 1.044454806070002, b 0.018661707292026614, cost 31.81101728851003, iteration 89\n",
      "m 1.0445365444770032, b 0.01870641019091935, cost 31.81100555754639, iteration 90\n",
      "m 1.0444623085750486, b 0.01874891115841746, cost 31.810994278775315, iteration 91\n",
      "m 1.0445275101511837, b 0.018793378708828794, cost 31.810983361481625, iteration 92\n",
      "m 1.0444680578498022, b 0.01883608628610563, cost 31.810972733166256, iteration 93\n",
      "m 1.044520043279852, b 0.018880365345844474, cost 31.810962335888206, iteration 94\n",
      "m 1.0444724059630832, b 0.01892323764326849, cost 31.810952123341476, iteration 95\n",
      "m 1.0445138291215608, b 0.0189673656608776, cost 31.810942058518464, iteration 96\n",
      "m 1.0444756342865145, b 0.01901036920048514, cost 31.810932111842874, iteration 97\n",
      "m 1.0445086161365338, b 0.01905437610466928, cost 31.81092225967744, iteration 98\n",
      "m 1.0444779676908766, b 0.01909748413105923, cost 31.81091248313143, iteration 99\n",
      "Using gradient descent function: Coef 1.0444779676908766 Intercept 0.01909748413105923\n",
      "Using sklearn: Coef [1.01773624] Intercept 1.9152193111569176\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    df = pd.read_csv(\"test_scores.csv\")\n",
    "    x = np.array(df.math)\n",
    "    y = np.array(df.cs)\n",
    "\n",
    "    m, b = gradient_descent(x,y)\n",
    "    print(\"Using gradient descent function: Coef {} Intercept {}\".format(m, b))\n",
    "\n",
    "    m_sklearn, b_sklearn = predict_using_sklean()\n",
    "    print(\"Using sklearn: Coef {} Intercept {}\".format(m_sklearn,b_sklearn))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
